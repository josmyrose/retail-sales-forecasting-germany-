{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: Retail Sales Trend Analysis & Forecasting using German Federal Bank Data**"
      ],
      "metadata": {
        "id": "FkBQgD5gujfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 02 ‚Äì Data Cleaning & Feature Engineering"
      ],
      "metadata": {
        "id": "drS0-huvsRTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objective**\n",
        "\n",
        "Transform raw Bundesbank retail time series into a clean, analysis-ready dataset with features for downstream analysis and forecasting."
      ],
      "metadata": {
        "id": "9QWZsIiqupqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1Ô∏è‚É£ Imports & Config**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gbYvW6B8mk4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6vGuQcUFGxy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(\"Notebook02_Cleaning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2Ô∏è‚É£ Load Raw Dataset**"
      ],
      "metadata": {
        "id": "CIX3d0vwmyz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/retaildataanalysis/dataset/cleaned_data.csv\",\n",
        "\n",
        ")\n",
        "logger.info(f\"Raw dataset loaded: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns\")\n"
      ],
      "metadata": {
        "id": "Um32wAcgJEag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3Ô∏è‚É£ Cleaning Pipeline**"
      ],
      "metadata": {
        "id": "M6tSHlcv1vzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_pipeline(df_raw, logger):\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # Convert date\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    invalid_dates = df[\"date\"].isna().sum()\n",
        "    if invalid_dates > 0:\n",
        "        logger.warning(f\"{invalid_dates} rows have invalid dates\")\n",
        "\n",
        "    # Convert retail_index\n",
        "    df[\"retail_index\"] = pd.to_numeric(df[\"retail_index\"], errors=\"coerce\")\n",
        "    invalid_index = df[\"retail_index\"].isna().sum()\n",
        "    if invalid_index > 0:\n",
        "        logger.warning(f\"{invalid_index} retail_index values could not be converted\")\n",
        "\n",
        "    # Drop invalid core values (ENFORCEMENT)\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"date\", \"retail_index\"])\n",
        "    dropped = before - len(df)\n",
        "\n",
        "    if dropped > 0:\n",
        "        logger.info(f\"Dropped {dropped} rows due to invalid core values\")\n",
        "\n",
        "    # Sort and check monotonicity\n",
        "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    if not df[\"date\"].is_monotonic_increasing:\n",
        "        logger.error(\"Dates are not monotonic after sorting\")\n",
        "\n",
        "    # Negative values check (log only, do not auto-fix)\n",
        "    if (df[\"retail_index\"] < 0).any():\n",
        "        logger.warning(\"Negative retail_index values detected\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "CztfbZi8ZiFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_pipeline(df_raw, logger):\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # Convert date\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    invalid_dates = df[\"date\"].isna().sum()\n",
        "    if invalid_dates > 0:\n",
        "        logger.warning(f\"{invalid_dates} rows have invalid dates\")\n",
        "\n",
        "    # Convert retail_index\n",
        "    df[\"retail_index\"] = pd.to_numeric(df[\"retail_index\"], errors=\"coerce\")\n",
        "    invalid_index = df[\"retail_index\"].isna().sum()\n",
        "    if invalid_index > 0:\n",
        "        logger.warning(f\"{invalid_index} retail_index values could not be converted\")\n",
        "\n",
        "    # Drop invalid core values (ENFORCEMENT)\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"date\", \"retail_index\"])\n",
        "    dropped = before - len(df)\n",
        "\n",
        "    if dropped > 0:\n",
        "        logger.info(f\"Dropped {dropped} rows due to invalid core values\")\n",
        "\n",
        "    # üîπ Enforce unique monthly index\n",
        "    if df[\"date\"].duplicated().any():\n",
        "        dup_count = df[\"date\"].duplicated().sum()\n",
        "        logger.warning(\n",
        "            f\"{dup_count} duplicate dates detected ‚Äî aggregating by mean\"\n",
        "        )\n",
        "\n",
        "        df = (\n",
        "            df\n",
        "            .groupby(\"date\", as_index=False)\n",
        "            .agg({\"retail_index\": \"mean\"})\n",
        "        )\n",
        "\n",
        "    # Sort and check monotonicity\n",
        "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    if not df[\"date\"].is_monotonic_increasing:\n",
        "        logger.error(\"Dates are not monotonic after sorting\")\n",
        "\n",
        "    # Negative values check (log only, do not auto-fix)\n",
        "    if (df[\"retail_index\"] < 0).any():\n",
        "        logger.warning(\"Negative retail_index values detected\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Fip31up5lv6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4Ô∏è‚É£ Feature Engineering Pipeline**"
      ],
      "metadata": {
        "id": "aXpj0loJ2QF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Time features\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['month_name'] = df['date'].dt.month_name()\n",
        "    df['quarter'] = df['date'].dt.quarter\n",
        "    df['day_of_week'] = df['date'].dt.day_name()\n",
        "\n",
        "    # Trend smoothing\n",
        "    df['rolling_12m_avg'] = df['retail_index'].rolling(12, min_periods=1).mean()\n",
        "    df['ema_12m'] = df['retail_index'].ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    # Growth metrics\n",
        "    df['yoy_growth_pct'] = df['retail_index'].pct_change(12) * 100\n",
        "    df['mom_growth_pct'] = df['retail_index'].pct_change(1) * 100\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "dGFsJjvMJF6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5Ô∏è‚É£ Run Pipelines**"
      ],
      "metadata": {
        "id": "zyAqzAlLKves"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = cleaning_pipeline(df_raw, logger)\n",
        "df_features = feature_engineering(df_clean)\n",
        "logger.info(\"Cleaning and feature engineering completed\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOnulYGFKzSg",
        "outputId": "0952d495-a355-4a7e-9a90-268cfb98fb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705334557.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
            "WARNING:Notebook02_Cleaning:4 rows have invalid dates\n",
            "WARNING:Notebook02_Cleaning:4 retail_index values could not be converted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6Ô∏è‚É£ HTML Validation Report**"
      ],
      "metadata": {
        "id": "YgHg70uZSSRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_validation_report(df, path=\"/content/drive/MyDrive/retaildataanalysis/dataset/outputs/validation_report.html\"):\n",
        "    report = pd.DataFrame({\n",
        "        \"column\": df.columns,\n",
        "        \"missing_values\": df.isna().sum(),\n",
        "        \"negative_values\": [(df[col] < 0).sum() if pd.api.types.is_numeric_dtype(df[col]) else 0 for col in df.columns]\n",
        "    })\n",
        "    report.to_html(path, index=False)\n",
        "    logger.info(f\"Validation report saved to {path}\")\n",
        "\n",
        "export_validation_report(df_features)\n"
      ],
      "metadata": {
        "id": "ya6wCmiYSbwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7Ô∏è‚É£ Unit Tests**"
      ],
      "metadata": {
        "id": "vjniZCF3Spbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cleaned_data(df):\n",
        "    assert df['retail_index'].notna().all(), \"Retail index has missing values\"\n",
        "    assert (df['retail_index'] >= 0).all(), \"Negative retail index values detected\"\n",
        "    assert df['date'].dtype == 'datetime64[ns]', \"Date column is not datetime\"\n",
        "\n",
        "def test_features(df):\n",
        "    for col in ['rolling_12m_avg', 'ema_12m', 'yoy_growth_pct', 'mom_growth_pct']:\n",
        "        assert col in df.columns, f\"{col} not found in DataFrame\"\n",
        "def test_unique_dates(df):\n",
        "    assert not df[\"date\"].duplicated().any(), \"Duplicate dates detected\"\n",
        "test_cleaned_data(df_features)\n",
        "test_features(df_features)\n",
        "test_unique_dates(df_features)\n",
        "logger.info(\"All unit tests passed ‚úÖ\")\n"
      ],
      "metadata": {
        "id": "kePXMhM2S8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8Ô∏è‚É£ Save Analysis-Ready Dataset**"
      ],
      "metadata": {
        "id": "cuGKPzFYTG4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_features.to_csv(\"/content/drive/MyDrive/retaildataanalysis/dataset/outputs/retail_analysis_ready.csv\", index=False)\n",
        "logger.info(\"Analysis-ready dataset saved for KPI analysis and forecasting\")\n"
      ],
      "metadata": {
        "id": "Uh-R9k1nTLWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}